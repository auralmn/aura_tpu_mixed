#!/usr/bin/env python3
# SPDX-License-Identifier: Apache-2.0

"""
MNIST learning test for AURA bio-inspired components
"""

import os
import sys
import json
import logging
import jax
import jax.numpy as jnp
import optax
from flax import linen as nn
from flax.training import train_state
from torchvision import datasets, transforms
import numpy as np

# Add src to path for imports
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Import bio-inspired components
from aura.bio_inspired.phasor_bank import PhasorBankJAX
from aura.bio_inspired.spiking_attention import SpikingAttentionJAX
from aura.bio_inspired.enhanced_spiking_retrieval import EnhancedSpikingRetrievalCore


class MNISTBioInspiredModel(nn.Module):
    """Model for MNIST classification using bio-inspired components"""
    hidden_dim: int = 256
    num_classes: int = 10
    
    def setup(self):
        # Initialize bio-inspired components
        self.phasor_bank = PhasorBankJAX(delta0=7.0, H=64)
        self.spiking_attention = SpikingAttentionJAX(k_winners=20, decay=0.7, theta=1.0)
        self.retrieval_core = EnhancedSpikingRetrievalCore(
            hidden_dim=self.hidden_dim,
            num_experts=8,
            expert_dim=128,
            phasor_harmonics=64
        )
        # Projection layers
        self.input_projection = nn.Dense(self.hidden_dim)
        self.output_projection = nn.Dense(self.num_classes)
    
    def __call__(self, x):
        # Flatten input image
        x_flat = x.reshape((x.shape[0], -1))  # [batch, 784]
        
        # Project to hidden dimension
        projected_x = self.input_projection(x_flat)
        
        # Extract temporal features using phasor bank
        # Use mean of each sample as input to phasor bank
        x_mean = jnp.mean(projected_x, axis=-1, keepdims=True)
        temporal_features = jax.vmap(self.phasor_bank)(x_mean.squeeze(-1))
        
        # Combine original input with temporal features
        enhanced_x = jnp.concatenate([projected_x, temporal_features], axis=-1)
        
        # Process through retrieval core
        context_vector = self.retrieval_core(enhanced_x)
        
        # Simple output projection
        logits = self.output_projection(context_vector)
        return logits


class MNISTLearningTest:
    """MNIST learning test with bio-inspired components"""
    
    def __init__(self, model_hidden_dim=256, num_classes=10):
        self.model_hidden_dim = model_hidden_dim
        self.num_classes = num_classes
        self.model = MNISTBioInspiredModel(
            hidden_dim=model_hidden_dim, 
            num_classes=num_classes
        )
        
        # Load MNIST dataset
        self.load_mnist_data()
        
    def load_mnist_data(self):
        """Load MNIST dataset"""
        try:
            # Load training data
            train_dataset = datasets.MNIST(
                root='./data', 
                train=True, 
                download=True,
                transform=transforms.ToTensor()
            )
            
            # Load test data
            test_dataset = datasets.MNIST(
                root='./data', 
                train=False, 
                download=True,
                transform=transforms.ToTensor()
            )
            
            # Convert to numpy arrays for JAX compatibility
            self.train_images = []
            self.train_labels = []
            for i in range(min(1000, len(train_dataset))):  # Use first 1000 samples
                image, label = train_dataset[i]
                self.train_images.append(image.numpy())
                self.train_labels.append(label)
            
            self.test_images = []
            self.test_labels = []
            for i in range(min(500, len(test_dataset))):  # Use first 500 samples
                image, label = test_dataset[i]
                self.test_images.append(image.numpy())
                self.test_labels.append(label)
            
            self.train_images = np.array(self.train_images).squeeze()
            self.train_labels = np.array(self.train_labels)
            self.test_images = np.array(self.test_images).squeeze()
            self.test_labels = np.array(self.test_labels)
            
            logger.info(f"Loaded MNIST data: {len(self.train_images)} training samples, {len(self.test_images)} test samples")
            
        except Exception as e:
            logger.error(f"Failed to load MNIST dataset: {e}")
            # Create synthetic data if MNIST loading fails
            self.create_synthetic_mnist_data()
    
    def create_synthetic_mnist_data(self):
        """Create synthetic MNIST-like data for testing"""
        logger.info("Creating synthetic MNIST data")
        
        # Create simple patterns for 10 classes
        self.train_images = np.random.rand(1000, 28, 28).astype(np.float32)
        self.train_labels = np.random.randint(0, 10, 1000)
        
        self.test_images = np.random.rand(500, 28, 28).astype(np.float32)
        self.test_labels = np.random.randint(0, 10, 500)
        
        # Add some simple patterns to make it learnable
        for i in range(1000):
            label = self.train_labels[i]
            # Add a simple pattern based on class
            self.train_images[i, label*2:(label*2+2), label*2:(label*2+2)] += 0.5
        
        for i in range(500):
            label = self.test_labels[i]
            # Add a simple pattern based on class
            self.test_images[i, label*2:(label*2+2), label*2:(label*2+2)] += 0.5
    
    def create_train_state(self, rng):
        """Create train state for the model"""
        # Create sample input to initialize model
        sample_x = jnp.ones((1, 28, 28))
        
        # Initialize model parameters
        params = self.model.init(rng, sample_x)
        
        # Create optimizer
        optimizer = optax.adam(learning_rate=1e-3)
        
        # Create train state
        state = train_state.TrainState.create(
            apply_fn=self.model.apply,
            params=params,
            tx=optimizer
        )
        
        return state
    
    @staticmethod
    @jax.jit
    def train_step(state, x_batch, y_batch):
        """Single training step with JIT compilation"""
        def loss_fn(params):
            logits = state.apply_fn(params, x_batch)
            loss = optax.softmax_cross_entropy_with_integer_labels(logits, y_batch).mean()
            return loss
        
        # Compute gradients
        loss, grads = jax.value_and_grad(loss_fn)(state.params)
        
        # Apply gradients
        state = state.apply_gradients(grads=grads)
        
        return state, loss
    
    @staticmethod
    @jax.jit
    def compute_accuracy(state, x_batch, y_batch):
        """Compute accuracy for current model state"""
        logits = state.apply_fn(state.params, x_batch)
        predictions = jnp.argmax(logits, axis=-1)
        accuracy = jnp.mean(predictions == y_batch)
        return accuracy
    
    def run_mnist_learning_test(self, target_accuracy=0.80, max_epochs=200):
        """Run MNIST learning test until target accuracy is reached"""
        logger.info(f"Starting MNIST learning test - target accuracy: {target_accuracy*100:.1f}%")
        
        # Create random key
        key = jax.random.PRNGKey(0)
        subkey = jax.random.split(key, 1)[0]
        
        # Create train state
        state = self.create_train_state(subkey)
        
        # Training loop
        epoch = 0
        best_accuracy = 0.0
        patience_counter = 0
        max_patience = 30  # Stop if no improvement for 30 epochs
        
        batch_size = 32
        num_batches = len(self.train_images) // batch_size
        
        while epoch < max_epochs:
            # Training epoch
            epoch_loss = 0.0
            for i in range(num_batches):
                # Get batch
                start_idx = i * batch_size
                end_idx = start_idx + batch_size
                x_batch = jnp.array(self.train_images[start_idx:end_idx])
                y_batch = jnp.array(self.train_labels[start_idx:end_idx])
                
                # Training step
                state, loss = self.train_step(state, x_batch, y_batch)
                epoch_loss += loss
            
            avg_loss = epoch_loss / num_batches
            
            # Evaluate every 5 epochs
            if epoch % 5 == 0:
                # Evaluate on test set
                test_batch_size = 50
                num_test_batches = len(self.test_images) // test_batch_size
                total_accuracy = 0.0
                
                for i in range(num_test_batches):
                    start_idx = i * test_batch_size
                    end_idx = start_idx + test_batch_size
                    x_test_batch = jnp.array(self.test_images[start_idx:end_idx])
                    y_test_batch = jnp.array(self.test_labels[start_idx:end_idx])
                    
                    accuracy = self.compute_accuracy(state, x_test_batch, y_test_batch)
                    total_accuracy += accuracy
                
                avg_accuracy = total_accuracy / num_test_batches
                
                if avg_accuracy > best_accuracy:
                    best_accuracy = avg_accuracy
                    patience_counter = 0
                else:
                    patience_counter += 1
                
                logger.info(f"Epoch {epoch}: Loss = {avg_loss:.4f}, Accuracy = {avg_accuracy:.4f}, Best = {best_accuracy:.4f}")
                
                # Check if target accuracy reached
                if avg_accuracy >= target_accuracy:
                    logger.info(f"üéØ Target accuracy {target_accuracy*100:.1f}% reached at epoch {epoch}")
                    return True
                
                # Check for early stopping
                if patience_counter >= max_patience:
                    logger.info(f"‚ö†Ô∏è No improvement for {max_patience} epochs, stopping early")
                    break
            
            epoch += 1
        
        logger.info(f"MNIST learning test completed. Best accuracy: {best_accuracy*100:.2f}%")
        
        if best_accuracy >= target_accuracy:
            logger.info(f"‚úÖ Model successfully reached target accuracy of {target_accuracy*100:.1f}%")
            return True
        else:
            logger.info(f"‚ùå Model did not reach target accuracy. Best was {best_accuracy*100:.2f}%")
            return False


def main():
    """Main function to run MNIST learning test"""
    # Create learning test
    learning_test = MNISTLearningTest(model_hidden_dim=256, num_classes=10)
    
    # Run test until 80% accuracy
    success = learning_test.run_mnist_learning_test(target_accuracy=0.80, max_epochs=200)
    
    if success:
        logger.info("MNIST learning test PASSED - 80% accuracy achieved")
        return True
    else:
        logger.error("MNIST learning test FAILED - 80% accuracy not achieved")
        return False


if __name__ == "__main__":
    success = main()
    if not success:
        sys.exit(1)
