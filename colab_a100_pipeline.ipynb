{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Consciousness-Aware SNN Training on Colab A100\n",
        "\n",
        "Use this notebook to fine-tune SBERT with SentencePiece-driven prosody features on a single A100 GPU. Cells install dependencies, stage data from GCS, build the PyTorch model, and run training with bfloat16 autocast plus gradient accumulation. Follow the configuration placeholders before launching a run.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify GPU availability\n",
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install runtime dependencies (PyTorch GPU build, Transformers, SentencePiece, etc.)\n",
        "%pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
        "%pip install --upgrade transformers accelerate sentencepiece datasets scikit-learn pandas tiktoken\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Authenticate and Configure GCS Access\n",
        "Run the next cell to authenticate with Google Cloud (required for `gsutil`). Ensure the Colab project has access to the `aura_tpu_data` bucket or update the placeholders to your bucket.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "\n",
        "# TODO: Update these values for your project/bucket layout\n",
        "PROJECT_ID = \"auragcloudtpu\"\n",
        "BUCKET_NAME = \"aura_tpu_data\"\n",
        "DATA_OBJECT = \"data/json/emotions.jsonl\"\n",
        "SP_MODEL_OBJECT = \"models/spm/spiece.model\"\n",
        "LOCAL_WORKDIR = pathlib.Path(\"/content/aura_tpu\")\n",
        "LOCAL_WORKDIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "os.environ[\"PROJECT_ID\"] = PROJECT_ID\n",
        "os.environ[\"BUCKET_NAME\"] = BUCKET_NAME\n",
        "os.environ[\"DATA_OBJECT\"] = DATA_OBJECT\n",
        "os.environ[\"SP_MODEL_OBJECT\"] = SP_MODEL_OBJECT\n",
        "print(f\"Configured workdir: {LOCAL_WORKDIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copy data and SentencePiece model from GCS\n",
        "!mkdir -p $LOCAL_WORKDIR/models/spm\n",
        "!gsutil -m cp gs://$BUCKET_NAME/$DATA_OBJECT $LOCAL_WORKDIR/\n",
        "!gsutil -m cp gs://$BUCKET_NAME/$SP_MODEL_OBJECT $LOCAL_WORKDIR/models/spm/spiece.model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sentencepiece as spm\n",
        "\n",
        "PLUTCHIK_LABELS = ['joy','trust','fear','surprise','sadness','disgust','anger','anticipation']\n",
        "COMPASS_INTENTS = ['inform','negotiate','question','clarify','social','express','command','request']\n",
        "INTENT_MAPPING = {\n",
        "    'share_news': 'inform','ask_help':'request','clarify':'clarify','complain':'express','thank':'social','propose':'negotiate'\n",
        "}\n",
        "PUNCTUATION_TOKENS = {'.','!','? ',',',';',';',':','...','!!','??'}\n",
        "\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "    data_path: str\n",
        "    sp_model_path: str\n",
        "    sbert_model_name: str = 'roberta-base'\n",
        "    max_length: int = 128\n",
        "    batch_size: int = 16\n",
        "    epochs: int = 5\n",
        "    learning_rate: float = 3e-5\n",
        "    weight_decay: float = 0.02\n",
        "    label_smoothing: float = 0.05\n",
        "    diversity_coef: float = 0.05\n",
        "    grad_accum_steps: int = 2\n",
        "    num_workers: int = 2\n",
        "    checkpoint_dir: str = '/content/aura_tpu/checkpoints'\n",
        "\n",
        "class ProductionSentencePieceLoader:\n",
        "    def __init__(self, model_path: str):\n",
        "        self.model_path = model_path\n",
        "        self.processor = spm.SentencePieceProcessor()\n",
        "        self.processor.load(model_path)\n",
        "        self.vocab_size = self.processor.get_piece_size()\n",
        "        print(f\"Loaded SentencePiece model: {model_path} (vocab={self.vocab_size})\")\n",
        "\n",
        "    def encode(self, text: str, max_len: int = 128):\n",
        "        ids = self.processor.encode(text, out_type=int)\n",
        "        pieces = self.processor.encode(text, out_type=str)\n",
        "        ids = ids[:max_len]\n",
        "        pieces = pieces[:max_len]\n",
        "        if len(ids) < max_len:\n",
        "            pad = max_len - len(ids)\n",
        "            ids += [self.processor.pad_id()] * pad\n",
        "            pieces += ['<pad>'] * pad\n",
        "        return ids, pieces\n",
        "\n",
        "class ProductionSentencePieceProsodyExtractor:\n",
        "    def __init__(self, punct_tokens: set = None):\n",
        "        self.punct_tokens = punct_tokens or PUNCTUATION_TOKENS\n",
        "\n",
        "    def prosody_features(self, pieces: List[str]):\n",
        "        max_len = len(pieces)\n",
        "        word_boundary = np.zeros(max_len, dtype=np.float32)\n",
        "        punctuation = np.zeros(max_len, dtype=np.float32)\n",
        "        subword_len = np.zeros(max_len, dtype=np.float32)\n",
        "        for i, token in enumerate(pieces):\n",
        "            normalized = token.replace('▁',' ')\n",
        "            word_boundary[i] = 1.0 if token.startswith('▁') else 0.0\n",
        "            punctuation[i] = 1.0 if normalized.strip() in self.punct_tokens else 0.0\n",
        "            subword_len[i] = float(len(normalized.strip()))\n",
        "        if subword_len.max() > 0:\n",
        "            subword_len = subword_len / subword_len.max()\n",
        "        return word_boundary, punctuation, subword_len\n",
        "\n",
        "class EmotionIntentDataset(Dataset):\n",
        "    def __init__(self, records: List[Dict], tokenizer: AutoTokenizer, sp_loader: ProductionSentencePieceLoader,\n",
        "                 prosody: ProductionSentencePieceProsodyExtractor, max_length: int = 128):\n",
        "        self.records = records\n",
        "        self.tokenizer = tokenizer\n",
        "        self.sp_loader = sp_loader\n",
        "        self.prosody = prosody\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.records)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        rec = self.records[idx]\n",
        "        text = (rec.get('text') or '').replace('\\u00A0', ' ')\n",
        "        tok = self.tokenizer(text, max_length=self.max_length, truncation=True, padding='max_length', return_tensors='pt')\n",
        "        input_ids = tok['input_ids'].squeeze(0)\n",
        "        attention_mask = tok['attention_mask'].squeeze(0)\n",
        "\n",
        "        sp_ids, sp_pieces = self.sp_loader.encode(text, max_len=128)\n",
        "        wb, pn, sl = self.prosody.prosody_features(sp_pieces)\n",
        "\n",
        "        plutchik = np.zeros(len(PLUTCHIK_LABELS), dtype=np.float32)\n",
        "        prim = rec.get('plutchik',{}).get('primary','joy')\n",
        "        intensity = float(rec.get('plutchik',{}).get('intensity',0.5))\n",
        "        if prim in PLUTCHIK_LABELS:\n",
        "            plutchik[PLUTCHIK_LABELS.index(prim)] = intensity\n",
        "        secondary = rec.get('plutchik',{}).get('secondary')\n",
        "        secondary_map = {\n",
        "            'optimism':'anticipation','admiration':'trust','anxiety':'fear','hope':'anticipation',\n",
        "            'excitement':'joy','contentment':'joy','grief':'sadness','despair':'sadness',\n",
        "            'contempt':'disgust','outrage':'anger','fury':'anger','resentment':'anger'\n",
        "        }\n",
        "        if secondary in secondary_map:\n",
        "            plutchik[PLUTCHIK_LABELS.index(secondary_map[secondary])] += 0.25\n",
        "        plutchik = plutchik / (plutchik.sum() + 1e-6)\n",
        "\n",
        "        mapped_intent = INTENT_MAPPING.get(rec.get('intent','inform'), rec.get('intent','inform'))\n",
        "        if mapped_intent not in COMPASS_INTENTS:\n",
        "            mapped_intent = 'inform'\n",
        "        intent = np.zeros(len(COMPASS_INTENTS), dtype=np.float32)\n",
        "        intent[COMPASS_INTENTS.index(mapped_intent)] = 1.0\n",
        "\n",
        "        style = rec.get('style', {})\n",
        "        beta = float(style.get('beta', 0.5))\n",
        "        phi = float(style.get('phi', 0.5))\n",
        "        urgency = float(intensity if intensity > 0.6 else intensity * 0.7)\n",
        "        certainty = float(phi if phi > 0 else 0.5)\n",
        "\n",
        "        sample = {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'sp_token_ids': torch.tensor(sp_ids, dtype=torch.long),\n",
        "            'sp_wb': torch.tensor(wb, dtype=torch.float32),\n",
        "            'sp_punct': torch.tensor(pn, dtype=torch.float32),\n",
        "            'sp_sublen': torch.tensor(sl, dtype=torch.float32),\n",
        "            'plutchik': torch.tensor(plutchik, dtype=torch.float32),\n",
        "            'intent': torch.tensor(intent, dtype=torch.float32),\n",
        "            'urgency': torch.tensor([urgency], dtype=torch.float32),\n",
        "            'certainty': torch.tensor([certainty], dtype=torch.float32),\n",
        "            'formality': torch.tensor([beta], dtype=torch.float32),\n",
        "            'politeness': torch.tensor([phi], dtype=torch.float32)\n",
        "        }\n",
        "        return sample\n",
        "\n",
        "def load_emotion_dataset(jsonl_path: str) -> List[Dict]:\n",
        "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
        "        return [json.loads(line) for line in f if line.strip()]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TorchSNN(nn.Module):\n",
        "    def __init__(self, sp_vocab: int, sbert_dim: int, num_experts: int = 4):\n",
        "        super().__init__()\n",
        "        self.sp_embed = nn.Embedding(sp_vocab, 128)\n",
        "        self.pause_dense = nn.Sequential(nn.Linear(128 + 7, 64), nn.GELU(), nn.Linear(64, 1))\n",
        "        self.stress_dense = nn.Sequential(nn.Linear(128 + 7, 64), nn.GELU(), nn.Linear(64, 1))\n",
        "        self.pitch_mlp = nn.Sequential(nn.Linear(3, 64), nn.GELU())\n",
        "        self.energy_mlp = nn.Sequential(nn.Linear(3, 64), nn.GELU())\n",
        "        self.emotion_head = nn.Sequential(nn.Linear(sbert_dim + 64 + 64, 128), nn.ReLU(), nn.Linear(128, 8))\n",
        "        self.intent_hidden = nn.Sequential(nn.Linear(sbert_dim + 128 + 64, 128), nn.ReLU())\n",
        "        self.intent_head = nn.Linear(128, 8)\n",
        "        self.modifier_head = nn.Linear(128, 4)\n",
        "        self.gate_head = nn.Linear(sbert_dim + 128 + 128 + 64 + 64, num_experts)\n",
        "        self.output_head = nn.Linear(sbert_dim + 128 + 128 + 64 + 64, 256)\n",
        "\n",
        "    def forward(self, sbert_emb: torch.Tensor, batch: Dict[str, torch.Tensor]):\n",
        "        sp_ids = batch['sp_token_ids']\n",
        "        sp_embed = self.sp_embed(sp_ids)  # [B,128,128]\n",
        "        wb = batch['sp_wb']\n",
        "        pn = batch['sp_punct']\n",
        "        sl = batch['sp_sublen']\n",
        "        sl_norm = sl / sl.amax(dim=1, keepdim=True).clamp(min=1.0)\n",
        "\n",
        "        # Compose per-token linguistic features\n",
        "        ling = torch.stack([\n",
        "            wb,\n",
        "            torch.roll(wb, shifts=1, dims=1),\n",
        "            torch.roll(wb, shifts=-1, dims=1),\n",
        "            pn,\n",
        "            torch.roll(pn, shifts=1, dims=1),\n",
        "            torch.roll(pn, shifts=-1, dims=1),\n",
        "            sl_norm\n",
        "        ], dim=-1)  # [B,128,7]\n",
        "        pause_in = torch.cat([sp_embed, ling], dim=-1)\n",
        "        pause_probs = torch.sigmoid(self.pause_dense(pause_in)).squeeze(-1)\n",
        "        stress_in = torch.cat([sp_embed, ling], dim=-1)\n",
        "        stress_probs = torch.sigmoid(self.stress_dense(stress_in)).squeeze(-1)\n",
        "\n",
        "        # Aggregate sentence-level prosody features\n",
        "        pitch = self.pitch_mlp(torch.stack([\n",
        "            stress_probs.std(dim=1),\n",
        "            stress_probs.mean(dim=1),\n",
        "            wb.mean(dim=1)\n",
        "        ], dim=-1))\n",
        "        energy = self.energy_mlp(torch.stack([\n",
        "            stress_probs.sum(dim=1),\n",
        "            pause_probs.sum(dim=1),\n",
        "            pn.sum(dim=1)\n",
        "        ], dim=-1))\n",
        "\n",
        "        emotion_h = torch.cat([sbert_emb, pitch, energy], dim=-1)\n",
        "        emo_logits = self.emotion_head(emotion_h)\n",
        "\n",
        "        intent_h_input = torch.cat([sbert_emb, F.relu(self.emotion_head[0](emotion_h)), pitch], dim=-1)\n",
        "        intent_hidden = self.intent_hidden(intent_h_input)\n",
        "        intent_logits = self.intent_head(intent_hidden)\n",
        "        modifiers = torch.sigmoid(self.modifier_head(intent_hidden))\n",
        "\n",
        "        composite = torch.cat([sbert_emb, intent_hidden, F.relu(self.emotion_head[0](emotion_h)), pitch, energy], dim=-1)\n",
        "        gate_weights = torch.softmax(self.gate_head(composite), dim=-1)\n",
        "        routed = self.output_head(composite)\n",
        "        return {\n",
        "            'emo_logits': emo_logits,\n",
        "            'intent_logits': intent_logits,\n",
        "            'modifiers': modifiers,\n",
        "            'gate': gate_weights,\n",
        "            'prosody': {\n",
        "                'pause': pause_probs,\n",
        "                'stress': stress_probs,\n",
        "                'pitch': pitch,\n",
        "                'energy': energy\n",
        "            },\n",
        "            'output': routed\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def smooth_labels(labels: torch.Tensor, smoothing: float) -> torch.Tensor:\n",
        "    if smoothing <= 0:\n",
        "        return labels\n",
        "    num_classes = labels.size(-1)\n",
        "    return (1.0 - smoothing) * labels + smoothing / num_classes\n",
        "\n",
        "\n",
        "def cross_entropy_with_probs(logits: torch.Tensor, probs: torch.Tensor) -> torch.Tensor:\n",
        "    log_probs = F.log_softmax(logits, dim=-1)\n",
        "    return F.kl_div(log_probs, probs, reduction='batchmean')\n",
        "\n",
        "\n",
        "def collate_batch(batch_list: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n",
        "    out = {}\n",
        "    for key in batch_list[0].keys():\n",
        "        out[key] = torch.stack([sample[key] for sample in batch_list], dim=0)\n",
        "    return out\n",
        "\n",
        "\n",
        "def prepare_dataloaders(cfg: TrainConfig, tokenizer: AutoTokenizer, sp_loader: ProductionSentencePieceLoader):\n",
        "    records = load_emotion_dataset(cfg.data_path)\n",
        "    train_records, temp_records = train_test_split(records, test_size=0.2, random_state=42)\n",
        "    val_records, test_records = train_test_split(temp_records, test_size=0.5, random_state=42)\n",
        "\n",
        "    prosody = ProductionSentencePieceProsodyExtractor()\n",
        "    train_dataset = EmotionIntentDataset(train_records, tokenizer, sp_loader, prosody, max_length=cfg.max_length)\n",
        "    val_dataset = EmotionIntentDataset(val_records, tokenizer, sp_loader, prosody, max_length=cfg.max_length)\n",
        "    test_dataset = EmotionIntentDataset(test_records, tokenizer, sp_loader, prosody, max_length=cfg.max_length)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True,\n",
        "                              num_workers=cfg.num_workers, pin_memory=True, collate_fn=collate_batch)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=cfg.batch_size, shuffle=False,\n",
        "                            num_workers=cfg.num_workers, pin_memory=True, collate_fn=collate_batch)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False,\n",
        "                             num_workers=cfg.num_workers, pin_memory=True, collate_fn=collate_batch)\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "\n",
        "def train_epoch(model: TorchSNN, sbert: AutoModel, loader: DataLoader, optim: torch.optim.Optimizer,\n",
        "                cfg: TrainConfig, device: torch.device) -> float:\n",
        "    model.train(); sbert.train()\n",
        "    total_loss = 0.0\n",
        "    step = 0\n",
        "    optim.zero_grad(set_to_none=True)\n",
        "    accum_steps = max(1, cfg.grad_accum_steps)\n",
        "    for batch in loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        with torch.cuda.amp.autocast(dtype=torch.bfloat16, enabled=device.type == 'cuda'):\n",
        "            sbert_outputs = sbert(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
        "            hidden = sbert_outputs.last_hidden_state\n",
        "            mask = batch['attention_mask'].float()\n",
        "            denom = mask.sum(dim=1, keepdim=True).clamp(min=1.0)\n",
        "            pooled = (hidden * mask.unsqueeze(-1)).sum(dim=1) / denom\n",
        "            outputs = model(pooled, batch)\n",
        "            emo_target = smooth_labels(batch['plutchik'], cfg.label_smoothing)\n",
        "            intent_target = smooth_labels(batch['intent'], cfg.label_smoothing)\n",
        "            emo_loss = cross_entropy_with_probs(outputs['emo_logits'], emo_target)\n",
        "            intent_loss = cross_entropy_with_probs(outputs['intent_logits'], intent_target)\n",
        "            mods = outputs['modifiers']\n",
        "            mod_loss = F.mse_loss(mods[:, 0:1], batch['urgency']) + \\\n",
        "                       F.mse_loss(mods[:, 1:2], batch['certainty']) + \\\n",
        "                       F.mse_loss(mods[:, 2:3], batch['formality']) + \\\n",
        "                       F.mse_loss(mods[:, 3:4], batch['politeness'])\n",
        "            gate_div = -(outputs['gate'] * outputs['gate'].clamp(min=1e-8).log()).sum(dim=-1).mean()\n",
        "            loss = emo_loss + intent_loss + 0.5 * mod_loss + cfg.diversity_coef * gate_div\n",
        "        loss.backward()\n",
        "        step += 1\n",
        "        if step % accum_steps == 0:\n",
        "            optim.step()\n",
        "            optim.zero_grad(set_to_none=True)\n",
        "        total_loss += float(loss.detach().cpu())\n",
        "    if step % accum_steps != 0:\n",
        "        optim.step()\n",
        "        optim.zero_grad(set_to_none=True)\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "\n",
        "def evaluate(model: TorchSNN, sbert: AutoModel, loader: DataLoader, cfg: TrainConfig, device: torch.device) -> float:\n",
        "    model.eval(); sbert.eval()\n",
        "    losses = []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            with torch.cuda.amp.autocast(dtype=torch.bfloat16, enabled=device.type == 'cuda'):\n",
        "                sbert_outputs = sbert(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
        "                hidden = sbert_outputs.last_hidden_state\n",
        "                mask = batch['attention_mask'].float()\n",
        "                denom = mask.sum(dim=1, keepdim=True).clamp(min=1.0)\n",
        "                pooled = (hidden * mask.unsqueeze(-1)).sum(dim=1) / denom\n",
        "                outputs = model(pooled, batch)\n",
        "                emo_target = smooth_labels(batch['plutchik'], cfg.label_smoothing)\n",
        "                intent_target = smooth_labels(batch['intent'], cfg.label_smoothing)\n",
        "                emo_loss = cross_entropy_with_probs(outputs['emo_logits'], emo_target)\n",
        "                intent_loss = cross_entropy_with_probs(outputs['intent_logits'], intent_target)\n",
        "                mods = outputs['modifiers']\n",
        "                mod_loss = F.mse_loss(mods[:, 0:1], batch['urgency']) + \\\n",
        "                           F.mse_loss(mods[:, 1:2], batch['certainty']) + \\\n",
        "                           F.mse_loss(mods[:, 2:3], batch['formality']) + \\\n",
        "                           F.mse_loss(mods[:, 3:4], batch['politeness'])\n",
        "                gate_div = -(outputs['gate'] * outputs['gate'].clamp(min=1e-8).log()).sum(dim=-1).mean()\n",
        "                loss = emo_loss + intent_loss + 0.5 * mod_loss + cfg.diversity_coef * gate_div\n",
        "            losses.append(float(loss.detach().cpu()))\n",
        "    return float(np.mean(losses)) if losses else 0.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pathlib\n",
        "from pathlib import Path\n",
        "\n",
        "def run_training():\n",
        "    cfg = TrainConfig(\n",
        "        data_path=str(LOCAL_WORKDIR / pathlib.Path(DATA_OBJECT).name),\n",
        "        sp_model_path=str(LOCAL_WORKDIR / \"models/spm/spiece.model\"),\n",
        "        sbert_model_name='roberta-large',\n",
        "        max_length=128,\n",
        "        batch_size=12,\n",
        "        epochs=8,\n",
        "        learning_rate=2e-5,\n",
        "        weight_decay=0.02,\n",
        "        label_smoothing=0.05,\n",
        "        diversity_coef=0.05,\n",
        "        grad_accum_steps=4,\n",
        "        num_workers=2,\n",
        "        checkpoint_dir=str(LOCAL_WORKDIR / \"checkpoints\")\n",
        "    )\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(cfg.sbert_model_name, use_fast=True)\n",
        "    sbert = AutoModel.from_pretrained(cfg.sbert_model_name)\n",
        "    if hasattr(sbert, 'gradient_checkpointing_enable'):\n",
        "        sbert.gradient_checkpointing_enable()\n",
        "    sbert.to(device)\n",
        "\n",
        "    sp_loader = ProductionSentencePieceLoader(cfg.sp_model_path)\n",
        "    train_loader, val_loader, test_loader = prepare_dataloaders(cfg, tokenizer, sp_loader)\n",
        "    model = TorchSNN(sp_vocab=sp_loader.vocab_size, sbert_dim=sbert.config.hidden_size, num_experts=4).to(device)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(list(model.parameters()) + list(sbert.parameters()),\n",
        "                                  lr=cfg.learning_rate, weight_decay=cfg.weight_decay)\n",
        "\n",
        "    Path(cfg.checkpoint_dir).mkdir(parents=True, exist_ok=True)\n",
        "    best_val = float('inf')\n",
        "    best_ckpt = None\n",
        "\n",
        "    for epoch in range(cfg.epochs):\n",
        "        print(f\"Epoch {epoch+1}/{cfg.epochs}\")\n",
        "        train_loss = train_epoch(model, sbert, train_loader, optimizer, cfg, device)\n",
        "        val_loss = evaluate(model, sbert, val_loader, cfg, device)\n",
        "        print(f\"  train_loss={train_loss:.4f} val_loss={val_loss:.4f}\")\n",
        "        if val_loss < best_val:\n",
        "            best_val = val_loss\n",
        "            best_ckpt = Path(cfg.checkpoint_dir) / f\"ckpt_epoch_{epoch+1:04d}.pt\"\n",
        "            torch.save({\n",
        "                'model_state': model.state_dict(),\n",
        "                'sbert_state': sbert.state_dict(),\n",
        "                'config': cfg.__dict__,\n",
        "                'val_loss': val_loss\n",
        "            }, best_ckpt)\n",
        "            print(f\"  Saved checkpoint: {best_ckpt}\")\n",
        "\n",
        "    test_loss = evaluate(model, sbert, test_loader, cfg, device)\n",
        "    print(f\"Test loss: {test_loss:.4f}\")\n",
        "    return cfg, best_ckpt\n",
        "\n",
        "cfg, best_checkpoint = run_training()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload best checkpoint back to GCS (optional)\n",
        "import subprocess\n",
        "\n",
        "if best_checkpoint is not None:\n",
        "    target_object = f\"checkpoints/{Path(best_checkpoint).name}\"\n",
        "    destination = f\"gs://{BUCKET_NAME}/{target_object}\"\n",
        "    cmd = [\"gsutil\", \"cp\", str(best_checkpoint), destination]\n",
        "    print(\" \".join(cmd))\n",
        "    subprocess.run(cmd, check=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Next Steps\n",
        "- Adjust `TrainConfig` hyperparameters (epochs, accumulation, batch) to match A100 budget.\n",
        "- Swap `sbert_model_name` to another HuggingFace checkpoint if needed.\n",
        "- Monitor GPU memory with `torch.cuda.max_memory_allocated()` if you push to larger batch sizes.\n",
        "- Extend the notebook with evaluation or export logic specific to your consciousness-aware routing pipeline.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
