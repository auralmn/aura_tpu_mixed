{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CONSCIOUSNESS-AWARE PROSODY–EMOTION SNN SYSTEM\n",
        "\n",
        "This notebook targets Google Colab with TPU v6 (Trillium). It implements a 12-cell pipeline: env setup, imports, mapping, prosody/emotion modules, dataset, integrated model, training, and inference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "CONSCIOUSNESS-AWARE PROSODY–EMOTION SNN SYSTEM\n",
        "Google Colab TPU v6 Implementation\n",
        "\"\"\"\n",
        "\n",
        "# TPU detection and env setup (Colab)\n",
        "import os\n",
        "import jax\n",
        "\n",
        "print(f\"JAX version: {jax.__version__}\")\n",
        "print(f\"Available devices: {jax.devices()}\")\n",
        "print(f\"TPU cores: {jax.device_count()}\")\n",
        "\n",
        "if jax.devices():\n",
        "    print(f\"First device: {jax.devices()[0]}\")\n",
        "\n",
        "# In Colab, uncomment to install packages\n",
        "# !pip -q install flax optax datasets transformers spacy wandb\n",
        "# !python -m spacy download en_core_web_sm\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random, jit\n",
        "from flax import linen as nn\n",
        "from flax.training import train_state, checkpoints\n",
        "import optax\n",
        "from typing import Dict, Optional\n",
        "\n",
        "print(\"✓ Imports ready\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'jnp' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      2\u001b[39m GOEMOTION_LABELS = [\n\u001b[32m      3\u001b[39m     \u001b[33m'\u001b[39m\u001b[33madmiration\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mamusement\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33manger\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mannoyance\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mapproval\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mcaring\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mconfusion\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mcuriosity\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mdesire\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mdisappointment\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      4\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mdisapproval\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mdisgust\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33membarrassment\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mexcitement\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mfear\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mgratitude\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mgrief\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mjoy\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mlove\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mnervousness\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33moptimism\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      5\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mpride\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mrealization\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mrelief\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mremorse\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33msadness\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33msurprise\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mneutral\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      6\u001b[39m ]\n\u001b[32m      7\u001b[39m PLUTCHIK_LABELS = [\u001b[33m'\u001b[39m\u001b[33mjoy\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mtrust\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mfear\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33msurprise\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33msadness\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mdisgust\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33manger\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33manticipation\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m GOEMOTION_TO_PLUTCHIK = \u001b[43mjnp\u001b[49m.array([\n\u001b[32m      9\u001b[39m   [\u001b[32m0\u001b[39m,\u001b[32m1\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m], [\u001b[32m1\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m], [\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m1\u001b[39m,\u001b[32m0\u001b[39m], [\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0.7\u001b[39m,\u001b[32m0\u001b[39m],\n\u001b[32m     10\u001b[39m   [\u001b[32m0\u001b[39m,\u001b[32m1\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m], [\u001b[32m0\u001b[39m,\u001b[32m0.8\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m], [\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0.5\u001b[39m,\u001b[32m0.3\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m], [\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0.5\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0.5\u001b[39m],\n\u001b[32m     11\u001b[39m   [\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m1\u001b[39m], [\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m1\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m], [\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0.8\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m], [\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m1\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m],\n\u001b[32m     12\u001b[39m   [\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0.3\u001b[39m,\u001b[32m0.7\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m], [\u001b[32m1\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m], [\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m1\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m], [\u001b[32m0.8\u001b[39m,\u001b[32m0.2\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m],\n\u001b[32m     13\u001b[39m   [\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m1\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m], [\u001b[32m1\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m], [\u001b[32m0.6\u001b[39m,\u001b[32m0.4\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m], [\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0.8\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m],\n\u001b[32m     14\u001b[39m   [\u001b[32m0.5\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0.5\u001b[39m], [\u001b[32m1\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m], [\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m1\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m], [\u001b[32m0.8\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m],\n\u001b[32m     15\u001b[39m   [\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m1\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m], [\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m1\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m], [\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m1\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m], [\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m]\n\u001b[32m     16\u001b[39m ], dtype=jnp.float32)\n\u001b[32m     18\u001b[39m \u001b[38;5;129m@jit\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmap_to_plutchik\u001b[39m(goemotion_probs: jnp.ndarray) -> jnp.ndarray:\n\u001b[32m     20\u001b[39m     p = jnp.matmul(goemotion_probs, GOEMOTION_TO_PLUTCHIK)\n",
            "\u001b[31mNameError\u001b[39m: name 'jnp' is not defined"
          ]
        }
      ],
      "source": [
        "# GoEmotions → Plutchik mapping\n",
        "GOEMOTION_LABELS = [\n",
        "    'admiration','amusement','anger','annoyance','approval','caring','confusion','curiosity','desire','disappointment',\n",
        "    'disapproval','disgust','embarrassment','excitement','fear','gratitude','grief','joy','love','nervousness','optimism',\n",
        "    'pride','realization','relief','remorse','sadness','surprise','neutral'\n",
        "]\n",
        "PLUTCHIK_LABELS = ['joy','trust','fear','surprise','sadness','disgust','anger','anticipation']\n",
        "GOEMOTION_TO_PLUTCHIK = jnp.array([\n",
        "  [0,1,0,0,0,0,0,0], [1,0,0,0,0,0,0,0], [0,0,0,0,0,0,1,0], [0,0,0,0,0,0,0.7,0],\n",
        "  [0,1,0,0,0,0,0,0], [0,0.8,0,0,0,0,0,0], [0,0,0.5,0.3,0,0,0,0], [0,0,0,0.5,0,0,0,0.5],\n",
        "  [0,0,0,0,0,0,0,1], [0,0,0,0,1,0,0,0], [0,0,0,0,0,0.8,0,0], [0,0,0,0,0,1,0,0],\n",
        "  [0,0,0,0,0.3,0.7,0,0], [1,0,0,0,0,0,0,0], [0,0,1,0,0,0,0,0], [0.8,0.2,0,0,0,0,0,0],\n",
        "  [0,0,0,0,1,0,0,0], [1,0,0,0,0,0,0,0], [0.6,0.4,0,0,0,0,0,0], [0,0,0.8,0,0,0,0,0],\n",
        "  [0.5,0,0,0,0,0,0,0.5], [1,0,0,0,0,0,0,0], [0,0,0,1,0,0,0,0], [0.8,0,0,0,0,0,0,0],\n",
        "  [0,0,0,0,1,0,0,0], [0,0,0,0,1,0,0,0], [0,0,0,1,0,0,0,0], [0,0,0,0,0,0,0,0]\n",
        "], dtype=jnp.float32)\n",
        "\n",
        "@jit\n",
        "def map_to_plutchik(goemotion_probs: jnp.ndarray) -> jnp.ndarray:\n",
        "    p = jnp.matmul(goemotion_probs, GOEMOTION_TO_PLUTCHIK)\n",
        "    return p / jnp.maximum(jnp.sum(p, axis=-1, keepdims=True), 1e-6)\n",
        "\n",
        "print(\"✓ Mapping ready\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ProsodyExtractorJAX(nn.Module):\n",
        "    \"\"\"Text-only prosody proxies: pitch, energy, duration, rhythm, pauses.\"\"\"\n",
        "    hidden_dim: int = 64\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self,\n",
        "                 token_embeddings: jnp.ndarray,  # [b, t, d]\n",
        "                 pos_tags: jnp.ndarray,          # [b, t, p]\n",
        "                 syntax_features: jnp.ndarray    # [b, t, s]\n",
        "                 ) -> Dict[str, jnp.ndarray]:\n",
        "        word_lengths = jnp.sum(jnp.abs(token_embeddings), axis=-1)  # [b,t]\n",
        "        pauses = nn.sigmoid(nn.Dense(1)(syntax_features)).squeeze(-1)  # [b,t]\n",
        "        stress = nn.sigmoid(nn.Dense(1)(pos_tags)).squeeze(-1)        # [b,t]\n",
        "        duration = nn.Dense(1)(jnp.stack([word_lengths, stress, pauses], axis=-1))  # [b,t,1]\n",
        "        pitch = nn.gelu(nn.Dense(self.hidden_dim)(jnp.stack([\n",
        "            jnp.mean(stress, axis=1), jnp.std(word_lengths, axis=1)\n",
        "        ], axis=-1)))\n",
        "        energy = nn.gelu(nn.Dense(self.hidden_dim)(jnp.stack([\n",
        "            jnp.sum(stress, axis=1), jnp.sum(pauses, axis=1)\n",
        "        ], axis=-1)))\n",
        "        rhythm_phase = 2 * jnp.pi * (jnp.mean(word_lengths, axis=1) / (jnp.max(word_lengths, axis=1) + 1e-6))\n",
        "        rhythm = jnp.stack([jnp.cos(rhythm_phase), jnp.sin(rhythm_phase)], axis=-1)  # [b,2]\n",
        "        return { 'pitch': pitch, 'energy': energy, 'duration': duration, 'rhythm': rhythm, 'pauses': pauses }\n",
        "\n",
        "class PlutchikEmotionEncoderJAX(nn.Module):\n",
        "    emotion_dim: int = 8\n",
        "    hidden_dim: int = 64\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self,\n",
        "                 text_embedding: jnp.ndarray,           # [b,d]\n",
        "                 prosody_features: Dict[str, jnp.ndarray],\n",
        "                 personality_traits: Optional[jnp.ndarray] = None  # [b,5]\n",
        "                 ) -> Dict[str, jnp.ndarray]:\n",
        "        semantic = nn.gelu(nn.Dense(self.hidden_dim)(text_embedding))\n",
        "        pros_concat = jnp.concatenate([\n",
        "            prosody_features['pitch'], prosody_features['energy'], prosody_features['rhythm']\n",
        "        ], axis=-1)\n",
        "        pros_h = nn.gelu(nn.Dense(self.hidden_dim)(pros_concat))\n",
        "        trait_h = nn.gelu(nn.Dense(self.hidden_dim)(personality_traits)) if personality_traits is not None else jnp.zeros_like(semantic)\n",
        "        fused = nn.gelu(nn.Dense(self.hidden_dim)(semantic * pros_h + trait_h))\n",
        "        goemotion_logits = nn.Dense(28)(fused)\n",
        "        goemotion = nn.sigmoid(goemotion_logits)\n",
        "        plutchik = map_to_plutchik(goemotion)\n",
        "        return { 'plutchik': plutchik, 'goemotion': goemotion, 'embeddings': fused }\n",
        "\n",
        "print(\"✓ Prosody & Emotion modules ready\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Minimal synthetic dataset loader (Colab: replace with HF datasets if desired)\n",
        "import numpy as np\n",
        "\n",
        "def make_synthetic_batch(batch_size: int = 8, seq_len: int = 64, embed_dim: int = 64):\n",
        "    key = random.PRNGKey(0)\n",
        "    token_embeddings = random.normal(key, (batch_size, seq_len, embed_dim))\n",
        "    pos_tags = random.normal(key, (batch_size, seq_len, 10))\n",
        "    syntax_features = random.normal(key, (batch_size, seq_len, 3))\n",
        "    goemotion_labels = jnp.clip(random.uniform(key, (batch_size, 28)), 0, 1)\n",
        "    # Normalize to probabilities for Plutchik labels (weak supervision)\n",
        "    plutchik_labels = map_to_plutchik(goemotion_labels)\n",
        "    return {\n",
        "        'token_embeddings': token_embeddings,\n",
        "        'pos_tags': pos_tags,\n",
        "        'syntax_features': syntax_features,\n",
        "        'goemotion_labels': goemotion_labels,\n",
        "        'plutchik_labels': plutchik_labels\n",
        "    }\n",
        "\n",
        "print(\"✓ Synthetic data function ready\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ConsciousnessAwareSNNModel(nn.Module):\n",
        "    embed_dim: int = 64\n",
        "    num_experts: int = 4\n",
        "    hidden_dim: int = 128\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self,\n",
        "                 token_embeddings: jnp.ndarray,\n",
        "                 pos_tags: jnp.ndarray,\n",
        "                 syntax_features: jnp.ndarray,\n",
        "                 personality_traits: Optional[jnp.ndarray] = None,\n",
        "                 training: bool = True):\n",
        "        b, t, d = token_embeddings.shape\n",
        "        pooled = jnp.mean(token_embeddings, axis=1)\n",
        "        pros = ProsodyExtractorJAX(hidden_dim=64)(token_embeddings, pos_tags, syntax_features)\n",
        "        emo = PlutchikEmotionEncoderJAX(hidden_dim=64)(pooled, pros, personality_traits)\n",
        "        composite = jnp.concatenate([\n",
        "            jnp.mean(pros['pitch'], axis=-1, keepdims=True),\n",
        "            jnp.mean(pros['energy'], axis=-1, keepdims=True),\n",
        "            emo['plutchik']\n",
        "        ], axis=-1)  # [b, 10]\n",
        "        gate = nn.softmax(nn.Dense(self.num_experts)(composite), axis=-1)\n",
        "        out = nn.gelu(nn.Dense(self.hidden_dim)(pooled))\n",
        "        return {'output': out, 'emotions': emo, 'prosody': pros, 'gate_weights': gate}\n",
        "\n",
        "print(\"✓ Integrated model ready\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SBERT setup (Colab: uncomment installs)\n",
        "# !pip -q install sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "SBERT_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"  # fast 384-dim\n",
        "sbert = SentenceTransformer(SBERT_MODEL_NAME)\n",
        "print(f\"Loaded SBERT: {SBERT_MODEL_NAME}\")\n",
        "\n",
        "def sbert_encode(texts):\n",
        "    \"\"\"Return SBERT embeddings as jnp array [batch, dim].\"\"\"\n",
        "    embs = sbert.encode(texts, convert_to_numpy=True, normalize_embeddings=False)\n",
        "    return jnp.asarray(embs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SBERTConsciousnessAwareSNN(nn.Module):\n",
        "    \"\"\"Consciousness-aware model using SBERT sentence embeddings.\"\"\"\n",
        "    sbert_dim: int = 384\n",
        "    num_experts: int = 4\n",
        "    hidden_dim: int = 128\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self,\n",
        "                 sbert_embeddings: jnp.ndarray,                 # [b, sbert_dim]\n",
        "                 personality_traits: Optional[jnp.ndarray] = None,\n",
        "                 training: bool = True):\n",
        "        # Project SBERT to model space\n",
        "        rich = nn.gelu(nn.Dense(self.hidden_dim)(sbert_embeddings))  # [b, hidden]\n",
        "\n",
        "        # Lightweight prosody proxies derived from SBERT\n",
        "        pitch = nn.gelu(nn.Dense(64)(rich))\n",
        "        energy = nn.gelu(nn.Dense(64)(rich))\n",
        "        rhythm = sbert_embeddings[:, :2] if sbert_embeddings.shape[1] >= 2 else jnp.zeros((sbert_embeddings.shape[0], 2))\n",
        "        pros = { 'pitch': pitch, 'energy': energy, 'rhythm': rhythm }\n",
        "\n",
        "        # Emotion via PlutchikEmotionEncoderJAX using SBERT as text embedding\n",
        "        emo = PlutchikEmotionEncoderJAX(hidden_dim=64)(sbert_embeddings, pros, personality_traits)\n",
        "\n",
        "        # Composite gating\n",
        "        comp = jnp.concatenate([\n",
        "            jnp.mean(pitch, axis=-1, keepdims=True),\n",
        "            jnp.mean(energy, axis=-1, keepdims=True),\n",
        "            emo['plutchik']\n",
        "        ], axis=-1)  # [b, 10]\n",
        "        gate = nn.softmax(nn.Dense(self.num_experts)(comp), axis=-1)\n",
        "\n",
        "        # Output head\n",
        "        out = nn.gelu(nn.Dense(self.hidden_dim)(rich))\n",
        "        return { 'output': out, 'emotions': emo, 'prosody': pros, 'gate_weights': gate }\n",
        "\n",
        "print(\"✓ SBERT-integrated model ready\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SBERT inference example\n",
        "texts = [\n",
        "    \"I am so excited and grateful for this amazing opportunity!\",\n",
        "    \"This is disappointing and makes me a bit sad.\"\n",
        "]\n",
        "embs = sbert_encode(texts)\n",
        "\n",
        "sbert_model = SBERTConsciousnessAwareSNN(sbert_dim=embs.shape[1])\n",
        "params = sbert_model.init({'params': random.PRNGKey(0)}, embs, None, False)\n",
        "outputs = sbert_model.apply(params, embs, None, False)\n",
        "\n",
        "for i, txt in enumerate(texts):\n",
        "    pl = outputs['emotions']['plutchik'][i]\n",
        "    print(f\"\\nText: {txt}\")\n",
        "    print(\"Plutchik:\")\n",
        "    for j, name in enumerate(['joy','trust','fear','surprise','sadness','disgust','anger','anticipation']):\n",
        "        print(f\"  {name:12s}: {float(pl[j]):.3f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fine-tune SBERT on GoEmotions → Plutchik (argmax pseudo-label)\n",
        "# !pip -q install datasets sentence-transformers\n",
        "from datasets import load_dataset\n",
        "from sentence_transformers import InputExample, losses\n",
        "from sentence_transformers import SentencesDataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "\n",
        "# Load dataset (small subset for demo)\n",
        "geo = load_dataset(\"go_emotions\", \"simplified\")\n",
        "train_split = geo[\"train\"].select(range(5000))\n",
        "\n",
        "# Build InputExamples with dominant Plutchik index as softmax label\n",
        "examples = []\n",
        "for ex in train_split:\n",
        "    text = ex[\"text\"]\n",
        "    # build multi-hot GoEmotions vector\n",
        "    ge = jnp.zeros((28,), dtype=jnp.float32).at[jnp.array(ex[\"labels\"])] .set(1.0)\n",
        "    # map to Plutchik distribution\n",
        "    pl = map_to_plutchik(ge[None, :])[0]\n",
        "    label_id = int(jnp.argmax(pl))\n",
        "    examples.append(InputExample(texts=[text], label=label_id))\n",
        "\n",
        "# SentenceTransformer expects a dataset and DataLoader\n",
        "train_dataset = SentencesDataset(examples, sbert)\n",
        "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=64)\n",
        "\n",
        "# Softmax loss over 8 Plutchik classes\n",
        "train_loss = losses.SoftmaxLoss(model=sbert, sentence_embedding_dimension=sbert.get_sentence_embedding_dimension(), num_labels=8)\n",
        "\n",
        "# Training\n",
        "warmup_steps = int(len(train_dataloader) * 1 * 0.1)\n",
        "print(f\"Training SBERT head with SoftmaxLoss over 8 classes, steps/epoch={len(train_dataloader)}\")\n",
        "sbert.fit(\n",
        "    train_objectives=[(train_dataloader, train_loss)],\n",
        "    epochs=1,\n",
        "    warmup_steps=warmup_steps,\n",
        "    show_progress_bar=True\n",
        ")\n",
        "print(\"✓ SBERT fine-tuning complete\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use fine-tuned SBERT in the pipeline\n",
        "texts_ft = [\n",
        "    \"I feel grateful and joyful today.\",\n",
        "    \"I am worried and a bit afraid of the outcome.\"\n",
        "]\n",
        "embs_ft = sbert_encode(texts_ft)\n",
        "outputs_ft = sbert_model.apply(params, embs_ft, None, False)\n",
        "\n",
        "for i, txt in enumerate(texts_ft):\n",
        "    pl = outputs_ft['emotions']['plutchik'][i]\n",
        "    print(f\"\\n[FT] Text: {txt}\")\n",
        "    for j, name in enumerate(['joy','trust','fear','surprise','sadness','disgust','anger','anticipation']):\n",
        "        print(f\"  {name:12s}: {float(pl[j]):.3f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bio-inspired multi-heads: STDP and NLMS\n",
        "class STDPHead(nn.Module):\n",
        "    \"\"\"Surrogate STDP head producing spike rates from a hidden state.\n",
        "    Loss encourages correlation between pre/post spikes given target sign.\n",
        "    \"\"\"\n",
        "    spike_dim: int = 64\n",
        "    v_th: float = 0.0\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, hidden: jnp.ndarray) -> jnp.ndarray:\n",
        "        v = nn.Dense(self.spike_dim)(hidden)\n",
        "        # Surrogate spike rate via fast sigmoid; center around threshold\n",
        "        spikes = jax.nn.sigmoid(v - self.v_th)\n",
        "        return spikes  # [b, spike_dim]\n",
        "\n",
        "    @staticmethod\n",
        "    def stdp_loss(pre: jnp.ndarray, post: jnp.ndarray, target_sign: jnp.ndarray) -> jnp.ndarray:\n",
        "        # Hebbian-like: encourage pre*post when target_sign>0, anti-Hebbian when <0\n",
        "        corr = jnp.mean(pre * post, axis=-1)\n",
        "        return jnp.mean(-target_sign * corr)\n",
        "\n",
        "class NLMSHead(nn.Module):\n",
        "    \"\"\"Normalized LMS predictor head.\n",
        "    Returns prediction y and computes NLMS residual-based loss.\n",
        "    \"\"\"\n",
        "    out_dim: int = 8  # align with Plutchik classes by default\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, hidden: jnp.ndarray) -> jnp.ndarray:\n",
        "        y = nn.Dense(self.out_dim)(hidden)\n",
        "        return y\n",
        "\n",
        "    @staticmethod\n",
        "    def nlms_loss(pred: jnp.ndarray, target: jnp.ndarray, x: jnp.ndarray, eps: float = 1e-6) -> jnp.ndarray:\n",
        "        # NLMS-inspired: minimize normalized squared error with input power\n",
        "        err = pred - target\n",
        "        norm = jnp.sum(x * x, axis=-1, keepdims=True) + eps\n",
        "        se = jnp.sum(err * err, axis=-1, keepdims=True) / norm\n",
        "        return jnp.mean(se)\n",
        "\n",
        "print(\"✓ STDP and NLMS heads ready\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SentencePiece adapter for SBERT (upload your .model to Colab first)\n",
        "# !pip -q install sentencepiece\n",
        "import sentencepiece as spm\n",
        "import torch\n",
        "\n",
        "class SentencePieceTokenizer:\n",
        "    def __init__(self, sp_model_path: str, max_seq_length: int = 128):\n",
        "        self.sp = spm.SentencePieceProcessor()\n",
        "        self.sp.load(sp_model_path)\n",
        "        self.pad_token_id = self.sp.pad_id()\n",
        "        self.cls_token_id = self.sp.bos_id()\n",
        "        self.sep_token_id = self.sp.eos_id()\n",
        "        self.unk_token_id = self.sp.unk_id()\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.vocab_size = self.sp.get_piece_size()\n",
        "\n",
        "    def __call__(self, texts, padding=True, truncation=True, max_length=None, return_tensors='pt'):\n",
        "        if isinstance(texts, str):\n",
        "            texts = [texts]\n",
        "        max_len = max_length or self.max_seq_length\n",
        "        ids_list = []\n",
        "        for txt in texts:\n",
        "            ids = self.sp.encode(txt, out_type=int)\n",
        "            if truncation:\n",
        "                ids = ids[:max_len - 2]\n",
        "            ids = [self.cls_token_id] + ids + [self.sep_token_id]\n",
        "            ids_list.append(ids)\n",
        "        if padding:\n",
        "            pad_len = max(len(ids) for ids in ids_list)\n",
        "            ids_list = [ids + [self.pad_token_id] * (pad_len - len(ids)) for ids in ids_list]\n",
        "        attn = [[1 if tok != self.pad_token_id else 0 for tok in ids] for ids in ids_list]\n",
        "        if return_tensors == 'pt':\n",
        "            return {\n",
        "                'input_ids': torch.tensor(ids_list, dtype=torch.long),\n",
        "                'attention_mask': torch.tensor(attn, dtype=torch.long)\n",
        "            }\n",
        "        return {'input_ids': ids_list, 'attention_mask': attn}\n",
        "\n",
        "print(\"✓ SentencePiece tokenizer wrapper ready\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prosody extraction from SentencePiece tokens\n",
        "class SentencePieceProsodyExtractor(nn.Module):\n",
        "    hidden_dim: int = 64\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self,\n",
        "                 token_ids: jnp.ndarray,           # [b, t]\n",
        "                 token_embeddings: jnp.ndarray     # [b, t, d]\n",
        "                 ) -> Dict[str, jnp.ndarray]:\n",
        "        b, t = token_ids.shape\n",
        "        # Simple heuristics: whitespace boundary ≈ small IDs; punct set example\n",
        "        word_boundary = (token_ids < 256).astype(jnp.float32)\n",
        "        punct_ids = jnp.array([33, 34, 35, 36, 37, 38, 39])  # placeholder ids\n",
        "        is_punct = jnp.isin(token_ids, punct_ids).astype(jnp.float32)\n",
        "        token_len = jnp.sum(jnp.abs(token_embeddings), axis=-1)\n",
        "\n",
        "        pause_feats = jnp.stack([\n",
        "            word_boundary,\n",
        "            is_punct,\n",
        "            jnp.roll(is_punct, 1, axis=1)\n",
        "        ], axis=-1)\n",
        "        pause_logits = nn.Dense(1)(pause_feats)\n",
        "        pause_probs = jax.nn.sigmoid(pause_logits).squeeze(-1)\n",
        "\n",
        "        stress_feats = jnp.stack([\n",
        "            word_boundary,\n",
        "            token_len,\n",
        "            jnp.abs(jnp.mean(token_embeddings, axis=-1))\n",
        "        ], axis=-1)\n",
        "        stress_logits = nn.Dense(1)(stress_feats)\n",
        "        stress_probs = jax.nn.sigmoid(stress_logits).squeeze(-1)\n",
        "\n",
        "        avg_stress = jnp.mean(stress_probs, axis=1)\n",
        "        stress_var = jnp.std(stress_probs, axis=1)\n",
        "        boundary_var = jnp.std(word_boundary, axis=1)\n",
        "\n",
        "        pitch = nn.gelu(nn.Dense(self.hidden_dim)(jnp.stack([avg_stress, stress_var, boundary_var], axis=-1)))\n",
        "        energy = nn.gelu(nn.Dense(self.hidden_dim)(jnp.stack([\n",
        "            jnp.sum(stress_probs, axis=1), jnp.sum(pause_probs, axis=1), jnp.sum(word_boundary, axis=1)\n",
        "        ], axis=-1)))\n",
        "\n",
        "        duration = nn.Dense(1)(jnp.stack([token_len, stress_probs, pause_probs], axis=-1))\n",
        "\n",
        "        return {\n",
        "            'pitch': pitch,\n",
        "            'energy': energy,\n",
        "            'duration': duration,\n",
        "            'rhythm': jnp.stack([boundary_var, stress_var], axis=-1),\n",
        "            'pauses': pause_probs,\n",
        "            'stress': stress_probs,\n",
        "            'word_boundaries': word_boundary,\n",
        "        }\n",
        "\n",
        "print(\"✓ SentencePiece prosody extractor ready\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo: SentencePiece → SBERT → Prosody → Emotion → Summary\n",
        "import os, glob\n",
        "from jax import random as jrandom\n",
        "\n",
        "# Locate .model under models/spm\n",
        "sp_search_dir = \"models/spm\"\n",
        "sp_model_candidates = glob.glob(os.path.join(sp_search_dir, \"**\", \"*.model\"), recursive=True)\n",
        "assert len(sp_model_candidates) > 0, f\"No .model found under {sp_search_dir}\"\n",
        "sp_model_path = sp_model_candidates[0]\n",
        "print(f\"Using SentencePiece model: {sp_model_path}\")\n",
        "\n",
        "# Build tokenizer\n",
        "sp_tokenizer = SentencePieceTokenizer(sp_model_path, max_seq_length=128)\n",
        "\n",
        "# Sample texts\n",
        "texts = [\n",
        "    \"I'm so happy! This is amazing.\",\n",
        "    \"I feel anxious, but trying to stay calm.\"\n",
        "]\n",
        "\n",
        "# Tokenize (PyTorch tensors from adapter; convert to numpy)\n",
        "sp_batch = sp_tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
        "input_ids = jnp.asarray(sp_batch['input_ids'].numpy())      # [b, t]\n",
        "attention_mask = jnp.asarray(sp_batch['attention_mask'].numpy())\n",
        "\n",
        "# SBERT sentence embeddings\n",
        "sent_embs = sbert_encode(texts)  # [b, dim]\n",
        "\n",
        "# Token-level embeddings (broadcast sentence embedding across sequence)\n",
        "b, t = input_ids.shape\n",
        "emb_dim = int(sent_embs.shape[1])\n",
        "token_embs = jnp.repeat(sent_embs[:, None, :], t, axis=1)   # [b, t, dim]\n",
        "\n",
        "# Prosody from SentencePiece tokens\n",
        "pros_extractor = SentencePieceProsodyExtractor(hidden_dim=64)\n",
        "pros_params = pros_extractor.init({'params': jrandom.PRNGKey(0)}, input_ids, token_embs)\n",
        "prosody = pros_extractor.apply(pros_params, input_ids, token_embs)\n",
        "\n",
        "# Emotions from SBERT embeddings + prosody\n",
        "emo_encoder = PlutchikEmotionEncoderJAX(hidden_dim=64)\n",
        "emo_params = emo_encoder.init({'params': jrandom.PRNGKey(1)}, sent_embs, prosody, None)\n",
        "emo = emo_encoder.apply(emo_params, sent_embs, prosody, None)\n",
        "\n",
        "# Print summary\n",
        "pl_labels = ['joy','trust','fear','surprise','sadness','disgust','anger','anticipation']\n",
        "for i, text in enumerate(texts):\n",
        "    print(\"\\nText:\", text)\n",
        "    # Show first 10 token pieces via underlying SP model\n",
        "    pieces = [sp_tokenizer.sp.id_to_piece(int(x)) for x in sp_batch['input_ids'][i][:10].tolist()]\n",
        "    print(\"Pieces:\", pieces)\n",
        "    print(\"Plutchik:\")\n",
        "    for j, name in enumerate(pl_labels):\n",
        "        print(f\"  {name:12s}: {float(emo['plutchik'][i, j]):.3f}\")\n",
        "    print(\"Prosody: pitch[0:5]=\", prosody['pitch'][i, :5])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Intent Compass module (primary intents + modifiers)\n",
        "INTENT_LABELS = ['inform','negotiate','question','clarify','social','express','command','request']\n",
        "INTENT_MODIFIERS = ['urgency','certainty','formality','politeness']\n",
        "\n",
        "class IntentCompassJAX(nn.Module):\n",
        "    hidden_dim: int = 128\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self,\n",
        "                 sbert_embedding: jnp.ndarray,        # [b, d]\n",
        "                 prosody_features: Dict[str, jnp.ndarray],\n",
        "                 emotion_probs: jnp.ndarray,          # [b, 8]\n",
        "                 personality_traits: jnp.ndarray | None = None\n",
        "                 ) -> Dict[str, jnp.ndarray]:\n",
        "        # semantic path\n",
        "        sem_h = nn.gelu(nn.Dense(self.hidden_dim)(sbert_embedding))\n",
        "        # prosody path\n",
        "        pros_concat = jnp.concatenate([\n",
        "            prosody_features['pitch'], prosody_features['energy'],\n",
        "            prosody_features['rhythm'] if prosody_features['rhythm'].ndim == 2 else prosody_features['rhythm'][:, :2],\n",
        "            jnp.mean(prosody_features.get('stress', prosody_features['pitch']), axis=-1, keepdims=True),\n",
        "            jnp.mean(prosody_features.get('pauses', prosody_features['energy']), axis=-1, keepdims=True)\n",
        "        ], axis=-1)\n",
        "        pros_h = nn.gelu(nn.Dense(self.hidden_dim)(pros_concat))\n",
        "        # emotion path\n",
        "        emo_h = nn.gelu(nn.Dense(self.hidden_dim)(emotion_probs))\n",
        "        # personality path\n",
        "        if personality_traits is not None:\n",
        "            pers_h = nn.gelu(nn.Dense(self.hidden_dim)(personality_traits))\n",
        "        else:\n",
        "            pers_h = jnp.zeros_like(sem_h)\n",
        "        # fuse\n",
        "        stack = jnp.stack([sem_h, pros_h, emo_h, pers_h], axis=1)  # [b,4,h]\n",
        "        w = nn.softmax(nn.Dense(1)(stack), axis=1)                  # [b,4,1]\n",
        "        fused = jnp.sum(stack * w, axis=1)\n",
        "        fused = nn.gelu(nn.Dense(self.hidden_dim)(fused))\n",
        "        # heads\n",
        "        primary_logits = nn.Dense(8)(fused)\n",
        "        primary = nn.softmax(primary_logits, axis=-1)\n",
        "        urgency = nn.sigmoid(nn.Dense(1)(jnp.concatenate([\n",
        "            jnp.mean(prosody_features['energy'], axis=-1, keepdims=True), emotion_probs[:, 6:7]\n",
        "        ], axis=-1)))\n",
        "        certainty = nn.sigmoid(nn.Dense(1)(jnp.concatenate([\n",
        "            1.0 - jnp.mean(prosody_features['pauses'], axis=-1, keepdims=True) if 'pauses' in prosody_features else jnp.ones((primary.shape[0],1)),\n",
        "            jnp.max(primary, axis=-1, keepdims=True)\n",
        "        ], axis=-1)))\n",
        "        formality = nn.sigmoid(nn.Dense(1)(fused[:, :2]))\n",
        "        politeness = nn.sigmoid(nn.Dense(1)(jnp.concatenate([emotion_probs[:, 1:2], fused[:, 2:3]], axis=-1)))\n",
        "        # compass position\n",
        "        angles = jnp.array([0, jnp.pi/4, jnp.pi/2, 3*jnp.pi/4, jnp.pi, 5*jnp.pi/4, 3*jnp.pi/2, 7*jnp.pi/4])\n",
        "        pos = jnp.stack([\n",
        "            jnp.sum(primary * jnp.cos(angles)[None, :], axis=-1),\n",
        "            jnp.sum(primary * jnp.sin(angles)[None, :], axis=-1)\n",
        "        ], axis=-1)\n",
        "        return {\n",
        "            'primary_intent': primary,\n",
        "            'modifiers': {\n",
        "                'urgency': urgency,\n",
        "                'certainty': certainty,\n",
        "                'formality': formality,\n",
        "                'politeness': politeness,\n",
        "            },\n",
        "            'compass_position': pos,\n",
        "        }\n",
        "\n",
        "print(\"✓ Intent Compass ready\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Intent Compass demo using previous SP+SBERT demo values\n",
        "ic = IntentCompassJAX(hidden_dim=128)\n",
        "ic_params = ic.init({'params': jrandom.PRNGKey(2)}, sent_embs, prosody, emo['plutchik'])\n",
        "ic_out = ic.apply(ic_params, sent_embs, prosody, emo['plutchik'])\n",
        "\n",
        "for i, text in enumerate(texts):\n",
        "    print(f\"\\nText: {text}\")\n",
        "    print(\"Primary intent distribution:\")\n",
        "    for j, name in enumerate(INTENT_LABELS):\n",
        "        print(f\"  {name:10s}: {float(ic_out['primary_intent'][i, j]):.3f}\")\n",
        "    mods = ic_out['modifiers']\n",
        "    print(\"Modifiers:\")\n",
        "    for m in INTENT_MODIFIERS:\n",
        "        print(f\"  {m:10s}: {float(mods[m][i, 0]):.3f}\")\n",
        "    cp = ic_out['compass_position'][i]\n",
        "    print(f\"Compass pos: ({float(cp[0]):.3f}, {float(cp[1]):.3f})\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "CELL 0: Mount/Upload emotions.jsonl (Colab)\n",
        "\"\"\"\n",
        "from google.colab import drive, files\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "print(\"Uploading emotions.jsonl...\")\n",
        "uploaded = files.upload()\n",
        "dataset_path = list(uploaded.keys())[0]\n",
        "print(f\"✓ File uploaded: {dataset_path} | size={os.path.getsize(dataset_path)/1024/1024:.2f} MB\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "CELL 1: Setup & Dependencies (TPU v6)\n",
        "\"\"\"\n",
        "import jax, jax.numpy as jnp\n",
        "from jax import random, jit\n",
        "from flax import linen as nn\n",
        "from flax.training import train_state\n",
        "import optax, numpy as np, json, os\n",
        "from datasets import Dataset\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "import wandb\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"CONSCIOUSNESS-AWARE SNN: EMOTION-INTENT TRAINING\")\n",
        "print(\"=\"*60)\n",
        "print(f\"TPU Cores: {jax.device_count()} | Device: {jax.devices()[0].device_kind} | JAX: {jax.__version__}\")\n",
        "\n",
        "run_name = f\"emotion_snn_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "wandb.init(project=\"consciousness-snn\", name=run_name,\n",
        "           config={\"batch_size_per_core\":32, \"epochs\":10, \"learning_rate\":3e-5})\n",
        "print(f\"✓ W&B initialized: {run_name}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "CELL 2: Load emotions.jsonl and split\n",
        "\"\"\"\n",
        "\n",
        "def load_emotion_dataset(jsonl_path):\n",
        "    recs = []\n",
        "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                try:\n",
        "                    recs.append(json.loads(line))\n",
        "                except json.JSONDecodeError:\n",
        "                    pass\n",
        "    return recs\n",
        "\n",
        "records = load_emotion_dataset(dataset_path)\n",
        "print(f\"✓ Loaded {len(records)} records\")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_records, temp_records = train_test_split(records, test_size=0.2, random_state=42)\n",
        "val_records, test_records = train_test_split(temp_records, test_size=0.5, random_state=42)\n",
        "print(f\"Split → Train {len(train_records)} | Val {len(val_records)} | Test {len(test_records)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "CELL 3: Label mappings & constants\n",
        "\"\"\"\n",
        "PLUTCHIK_LABELS = ['joy','trust','fear','surprise','sadness','disgust','anger','anticipation']\n",
        "COMPASS_INTENTS = ['inform','negotiate','question','clarify','social','express','command','request']\n",
        "\n",
        "# Example mapping from custom intents to compass intents (extend as needed)\n",
        "INTENT_MAPPING = {\n",
        "    'share_news': 'inform', 'ask_help': 'request', 'clarify': 'clarify',\n",
        "    'complain': 'express', 'thank': 'social', 'propose': 'negotiate'\n",
        "}\n",
        "\n",
        "TONE_TO_PROSODY = {\n",
        "    'ecstatic': {'energy': 0.95, 'pitch_var': 0.9, 'tempo': 1.3},\n",
        "    'urgent': {'energy': 0.9, 'pitch_var': 0.8, 'tempo': 1.4},\n",
        "    'neutral': {'energy': 0.5, 'pitch_var': 0.4, 'tempo': 1.0},\n",
        "}\n",
        "print(\"✓ Mappings ready\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "CELL 4: Preprocess all records with SBERT and lightweight linguistic features\n",
        "\"\"\"\n",
        "sbert_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "print(f\"✓ SBERT loaded: {sbert_model.get_sentence_embedding_dimension()}-dim\")\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "cache_dir = \"/tmp/sbert_cache\"; os.makedirs(cache_dir, exist_ok=True)\n",
        "\n",
        "def preprocess_record(record, idx):\n",
        "    text = record.get('text', '')\n",
        "    rid = record.get('id', f'rec_{idx}')\n",
        "    cf = f\"{cache_dir}/{rid}.npy\"\n",
        "    if os.path.exists(cf):\n",
        "        emb = np.load(cf)\n",
        "    else:\n",
        "        emb = sbert_model.encode(text, convert_to_tensor=False)\n",
        "        np.save(cf, emb)\n",
        "    doc = nlp(text)\n",
        "    token_ids = np.array([hash(t.text) % 32000 for t in doc[:128]], dtype=np.int32)\n",
        "    if len(token_ids) < 128:\n",
        "        token_ids = np.pad(token_ids, (0, 128 - len(token_ids)))\n",
        "    pos = np.zeros((128, 10), dtype=np.float32)\n",
        "    syn = np.zeros((128, 3), dtype=np.float32)\n",
        "    pm = {'NOUN':0,'VERB':1,'ADJ':2,'ADV':3,'PRON':4,'DET':5,'ADP':6,'CONJ':7,'NUM':8,'PUNCT':9}\n",
        "    for i, tok in enumerate(list(doc)[:128]):\n",
        "        if tok.pos_ in pm: pos[i, pm[tok.pos_]] = 1.0\n",
        "        syn[i,0] = min(abs(tok.head.i - tok.i),10)/10.0\n",
        "        syn[i,1] = 1.0 if tok.is_punct else 0.0\n",
        "        syn[i,2] = 1.0 if tok.is_stop else 0.0\n",
        "    # plutchik\n",
        "    p = np.zeros(8, dtype=np.float32)\n",
        "    prim = record.get('plutchik',{}).get('primary','joy')\n",
        "    inten = float(record.get('plutchik',{}).get('intensity',0.5))\n",
        "    if prim in PLUTCHIK_LABELS: p[PLUTCHIK_LABELS.index(prim)] = inten\n",
        "    sec = record.get('plutchik',{}).get('secondary')\n",
        "    sec_map = {'optimism':'anticipation','admiration':'trust','anxiety':'fear','hope':'anticipation','excitement':'joy','contentment':'joy','grief':'sadness','despair':'sadness','contempt':'disgust','outrage':'anger','fury':'anger','resentment':'anger'}\n",
        "    if sec in sec_map: p[PLUTCHIK_LABELS.index(sec_map[sec])] += 0.25\n",
        "    p = p / (np.sum(p)+1e-6)\n",
        "    # intent\n",
        "    mapped = INTENT_MAPPING.get(record.get('intent','inform'),'inform')\n",
        "    intent_idx = COMPASS_INTENTS.index(mapped)\n",
        "    intent_oh = np.zeros(8, dtype=np.float32); intent_oh[intent_idx]=1.0\n",
        "    # tone\n",
        "    tone = record.get('tone','neutral'); pros = TONE_TO_PROSODY.get(tone, TONE_TO_PROSODY['neutral'])\n",
        "    style = record.get('style',{}); beta = float(style.get('beta',0.5)); phi=float(style.get('phi',0.5))\n",
        "    urgency = inten if inten>0.6 else inten*0.7; certainty = phi if phi>0 else 0.5\n",
        "    return {\n",
        "        'sbert_embedding': emb.astype(np.float32),\n",
        "        'token_ids': token_ids,\n",
        "        'pos_tags': pos,\n",
        "        'syntax_features': syn,\n",
        "        'plutchik_probs': p,\n",
        "        'intent_label': intent_oh,\n",
        "        'urgency': urgency,\n",
        "        'certainty': certainty,\n",
        "        'formality': beta,\n",
        "        'politeness': phi,\n",
        "    }\n",
        "\n",
        "print(\"Preprocessing train/val/test...\")\n",
        "train_processed = [preprocess_record(r,i) for i,r in enumerate(tqdm(train_records))]\n",
        "val_processed   = [preprocess_record(r,i) for i,r in enumerate(tqdm(val_records))]\n",
        "test_processed  = [preprocess_record(r,i) for i,r in enumerate(tqdm(test_records))]\n",
        "print(\"✓ Preprocessing done\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "CELL 5: Model (emotion + intent + modifiers) and training loop\n",
        "\"\"\"\n",
        "class ConsciousnessAwareSNN(nn.Module):\n",
        "    num_experts: int = 5\n",
        "    hidden_dim: int = 256\n",
        "    sbert_dim: int = 384\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, sbert_embeddings, token_ids, pos_tags, syntax_features, training=True):\n",
        "        # Prosody (lightweight)\n",
        "        pauses = nn.sigmoid(nn.Dense(1)(nn.relu(nn.Dense(32)(syntax_features)))).squeeze(-1)\n",
        "        stress = nn.sigmoid(nn.Dense(1)(nn.relu(nn.Dense(32)(pos_tags)))).squeeze(-1)\n",
        "        pitch = nn.relu(nn.Dense(64)(jnp.concatenate([jnp.mean(stress, axis=1, keepdims=True), jnp.max(pauses, axis=1, keepdims=True)], axis=-1)))\n",
        "        energy = nn.relu(nn.Dense(64)(jnp.concatenate([jnp.std(stress, axis=1, keepdims=True), jnp.sum(pauses, axis=1, keepdims=True)], axis=-1)))\n",
        "        prosody = {'pitch': pitch, 'energy': energy, 'pauses': pauses, 'stress': stress}\n",
        "        # Emotion\n",
        "        emotion_h = nn.relu(nn.Dense(128)(jnp.concatenate([sbert_embeddings, pitch, energy], axis=-1)))\n",
        "        plutchik_probs = nn.softmax(nn.Dense(8)(emotion_h))\n",
        "        # Intent\n",
        "        intent_h = nn.relu(nn.Dense(128)(jnp.concatenate([sbert_embeddings, emotion_h, pitch], axis=-1)))\n",
        "        primary_intent = nn.softmax(nn.Dense(8)(intent_h))\n",
        "        # Modifiers\n",
        "        urgency = nn.sigmoid(nn.Dense(1)(intent_h)); certainty = nn.sigmoid(nn.Dense(1)(intent_h))\n",
        "        formality = nn.sigmoid(nn.Dense(1)(intent_h)); politeness = nn.sigmoid(nn.Dense(1)(intent_h))\n",
        "        # Experts\n",
        "        composite = jnp.concatenate([sbert_embeddings, emotion_h, intent_h], axis=-1)\n",
        "        gate_weights = nn.softmax(nn.Dense(self.num_experts)(composite))\n",
        "        output = nn.Dense(self.hidden_dim)(composite)\n",
        "        return {\n",
        "            'output': output,\n",
        "            'prosody': prosody,\n",
        "            'emotions': {'plutchik': plutchik_probs},\n",
        "            'intent': {\n",
        "                'primary_intent': primary_intent,\n",
        "                'modifiers': {'urgency': urgency, 'certainty': certainty, 'formality': formality, 'politeness': politeness}\n",
        "            },\n",
        "            'gate_weights': gate_weights\n",
        "        }\n",
        "\n",
        "# Init\n",
        "rng = random.PRNGKey(42)\n",
        "model = ConsciousnessAwareSNN()\n",
        "params = model.init({'params': rng}, jnp.ones((2,384)), jnp.ones((2,128),dtype=jnp.int32), jnp.ones((2,128,10)), jnp.ones((2,128,3)), training=False)['params']\n",
        "\n",
        "# Optimizer\n",
        "steps = (len(train_processed)//32)*10\n",
        "schedule = optax.warmup_cosine_decay_schedule(0.0, 3e-5, 50, steps, 1e-5)\n",
        "tx = optax.chain(optax.clip_by_global_norm(1.0), optax.adamw(learning_rate=schedule, weight_decay=0.01))\n",
        "state = train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
        "\n",
        "@jit\n",
        "def train_step(state, batch):\n",
        "    def loss_fn(p):\n",
        "        out = model.apply({'params': p}, batch['sbert_embedding'], batch['token_ids'], batch['pos_tags'], batch['syntax_features'], training=True)\n",
        "        el = optax.softmax_cross_entropy(out['emotions']['plutchik'], batch['plutchik_probs']).mean()\n",
        "        il = optax.softmax_cross_entropy(out['intent']['primary_intent'], batch['intent_label']).mean()\n",
        "        m = out['intent']['modifiers']; ml = ((m['urgency']-batch['urgency'])**2 + (m['certainty']-batch['certainty'])**2 + (m['formality']-batch['formality'])**2 + (m['politeness']-batch['politeness'])**2).mean()\n",
        "        gw = out['gate_weights']; div = -jnp.mean(jnp.sum(gw * jnp.log(gw + 1e-8), axis=-1))\n",
        "        total = 1.0*el + 1.0*il + 0.5*ml + 0.02*div\n",
        "        return total, {'loss': total, 'emotion': el, 'intent': il, 'modifiers': ml, 'diversity': -div}\n",
        "    (loss, metrics), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
        "    return state.apply_gradients(grads=grads), metrics\n",
        "\n",
        "def batches(data, bs=32, shuffle=True):\n",
        "    idx = np.arange(len(data));\n",
        "    if shuffle: np.random.shuffle(idx)\n",
        "    for s in range(0, len(idx), bs):\n",
        "        sel = idx[s:s+bs]; d=[data[i] for i in sel]\n",
        "        yield {\n",
        "            'sbert_embedding': jnp.array([x['sbert_embedding'] for x in d]),\n",
        "            'token_ids': jnp.array([x['token_ids'] for x in d]),\n",
        "            'pos_tags': jnp.array([x['pos_tags'] for x in d]),\n",
        "            'syntax_features': jnp.array([x['syntax_features'] for x in d]),\n",
        "            'plutchik_probs': jnp.array([x['plutchik_probs'] for x in d]),\n",
        "            'intent_label': jnp.array([x['intent_label'] for x in d]),\n",
        "            'urgency': jnp.array([x['urgency'] for x in d]).reshape(-1,1),\n",
        "            'certainty': jnp.array([x['certainty'] for x in d]).reshape(-1,1),\n",
        "            'formality': jnp.array([x['formality'] for x in d]).reshape(-1,1),\n",
        "            'politeness': jnp.array([x['politeness'] for x in d]).reshape(-1,1),\n",
        "        }\n",
        "\n",
        "print(\"✓ Training loop ready\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "CELL 6: Train on your dataset (10 epochs demo)\n",
        "\"\"\"\n",
        "print(\"\\n\"+\"=\"*60) ; print(\"START TRAINING\") ; print(\"=\"*60)\n",
        "for epoch in range(10):\n",
        "    metrics_buf = []\n",
        "    for step, batch in enumerate(batches(train_processed, bs=32, shuffle=True)):\n",
        "        state, metrics = train_step(state, batch)\n",
        "        metrics_buf.append(metrics)\n",
        "        if (step+1)%10==0:\n",
        "            avg = jnp.mean(jnp.array([m['loss'] for m in metrics_buf[-10:]]))\n",
        "            print(f\"  epoch {epoch+1} step {step+1}: loss={float(avg):.4f}\")\n",
        "    avg_epoch = jnp.mean(jnp.array([m['loss'] for m in metrics_buf]))\n",
        "    wandb.log({'epoch': epoch+1, 'train_loss': float(avg_epoch)})\n",
        "print(\"✓ Training complete\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
